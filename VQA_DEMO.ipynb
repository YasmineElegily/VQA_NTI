{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8_A58IVX8zu",
        "outputId": "36829ea5-e22c-44d4-8adc-297e4129db2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sahi\n",
            "  Downloading sahi-0.11.14-py3-none-any.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio\n",
            "  Downloading gradio-3.46.0-py3-none-any.whl (20.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python>=4.2.0.32 in /usr/local/lib/python3.10/dist-packages (from sahi) (4.8.0.76)\n",
            "Requirement already satisfied: shapely>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from sahi) (2.0.1)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.10/dist-packages (from sahi) (4.66.1)\n",
            "Requirement already satisfied: pillow>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from sahi) (9.4.0)\n",
            "Collecting pybboxes==0.1.6 (from sahi)\n",
            "  Downloading pybboxes-0.1.6-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from sahi) (6.0.1)\n",
            "Collecting fire (from sahi)\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting terminaltables (from sahi)\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from sahi) (2.31.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sahi) (8.1.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pybboxes==0.1.6->sahi) (1.23.5)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.103.2-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.5.3 (from gradio)\n",
            "  Downloading gradio_client-0.5.3-py3-none-any.whl (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.4/298.4 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio)\n",
            "  Downloading httpx-0.25.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.14.0 (from gradio)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.10.12)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.5.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<12.0,>=10.0 (from gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.5.3->gradio) (2023.6.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.12.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->sahi) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->sahi) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->sahi) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->sahi) (2023.7.22)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (3.7.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->sahi) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->sahi) (2.3.0)\n",
            "Collecting httpcore<0.19.0,>=0.18.0 (from httpx->gradio)\n",
            "  Downloading httpcore-0.18.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio) (1.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.10.2)\n",
            "Building wheels for collected packages: ffmpy, fire\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=a428e4febe733644a66a6748970e270c3d9ce948a19f64b5ba60ce3357b65d6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=03f220ae82e9c1410e96e7a2a8ff0380deb44787dd5eacd8b7c6ce88357d39f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "Successfully built ffmpy fire\n",
            "Installing collected packages: pydub, ffmpy, websockets, terminaltables, semantic-version, python-multipart, pybboxes, orjson, h11, fire, aiofiles, uvicorn, starlette, sahi, huggingface-hub, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.103.2 ffmpy-0.3.1 fire-0.5.0 gradio-3.46.0 gradio-client-0.5.3 h11-0.14.0 httpcore-0.18.0 httpx-0.25.0 huggingface-hub-0.17.3 orjson-3.9.7 pybboxes-0.1.6 pydub-0.25.1 python-multipart-0.0.6 sahi-0.11.14 semantic-version-2.10.0 starlette-0.27.0 terminaltables-3.1.10 uvicorn-0.23.2 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install sahi gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AknYFlOI4hb_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "L9Fhgv_SpwXx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "#from tensorflow import keras\n",
        "import random\n",
        "from tensorflow.keras import layers\n",
        "import keras\n",
        "import pandas as pd\n",
        "import tensorflow_datasets as tfds\n",
        "import cv2 as cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "N31awUl_qcsu"
      },
      "outputs": [],
      "source": [
        "\n",
        "encoder=tfds.deprecated.text.TokenTextEncoder.load_from_file('/content/drive/MyDrive/VQA_model_weights_v2/tmp2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DlE-TKjLtWlZ",
        "outputId": "ac1efed8-967c-4956-c18f-7d86931c4b64"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[9572, 5192, 22472]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoder.encode('what is this')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "o45Av5JRtgv6",
        "outputId": "119f6b9d-cb28-4c0c-f351-6a260cbd8c7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[26475]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoder.encode('kariim')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lxfFRirDxe_0",
        "outputId": "69abe643-8b7b-434d-9ba6-f3cc2303a48b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "26474"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "encoder.vocab_size-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6nXS_dSkB3dT"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32  # Batch size for training.\n",
        "EPOCHS = 10  # Number of epochs to train for.\n",
        "NUM_SAMPLES = 512  # Number of samples to train on.\n",
        "\n",
        "EMBEDDING_SIZE = 512\n",
        "FF_EXPANSION = 1024\n",
        "HEADS = 3\n",
        "\n",
        "VOCAB_SIZE = encoder.vocab_size-2\n",
        "\n",
        "#VOCAB_SIZE =100\n",
        "SEQUENCE_LENGTH = 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnHamfR7EuMN"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "m3nfjo53o8zp"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(length, depth):\n",
        "  \"\"\"\n",
        "  Generates a matrix of position encodings for an input sequence.\n",
        "\n",
        "  Args:\n",
        "      length: An integer representing the length of the input sequence.\n",
        "      depth: An integer representing the dimensionality of the encoding.\n",
        "\n",
        "  Returns:\n",
        "      A `tf.Tensor` of shape `(length, depth)` representing the position encoding matrix.\n",
        "  \"\"\"\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1)\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5N7RE_rwpbYd"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  This layer combines the input embedding with a positional encoding that helps the Transformer to understand\n",
        "  the relative position of the tokens in a sequence. It takes an input sequence of tokens and converts it to\n",
        "  a sequence of embedding vectors, then adds positional information to it.\n",
        "\n",
        "  Attributes:\n",
        "      vocab_size (int): The size of the vocabulary, i.e., the number of unique tokens in the input sequence.\n",
        "      d_model (int): The number of dimensions in the embedding vector.\n",
        "\n",
        "  Methods:\n",
        "      compute_mask(*args, **kwargs): This method computes the mask to be applied to the embeddings.\n",
        "      call(x): This method performs the computation for the layer.\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, vocab_size, d_model,maxlen):\n",
        "    \"\"\"\n",
        "    Initializes the PositionalEmbedding layer.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): The size of the vocabulary, i.e., the number of unique tokens in the input sequence.\n",
        "        d_model (int): The number of dimensions in the embedding vector.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Computes the mask to be applied to the embeddings.\n",
        "\n",
        "    Args:\n",
        "        *args: Variable length argument list.\n",
        "        **kwargs: Arbitrary keyword arguments.\n",
        "\n",
        "    Returns:\n",
        "        Mask to be applied to the embeddings.\n",
        "    \"\"\"\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Computes the output of the layer.\n",
        "\n",
        "    Args:\n",
        "        x (tf.Tensor): Input sequence of tokens.\n",
        "\n",
        "    Returns:\n",
        "        The output sequence of embedding vectors with added positional information.\n",
        "    \"\"\"\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "K-NZjKtuFGDs"
      },
      "outputs": [],
      "source": [
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  Base Attention layer class that contains a MultiHeadAttention, LayerNormalization and Add layer.\n",
        "\n",
        "  Attributes:\n",
        "  -----------\n",
        "  kwargs: dict\n",
        "      keyword arguments that will be passed to the MultiHeadAttention layer during initialization.\n",
        "\n",
        "  Methods:\n",
        "  --------\n",
        "  call(inputs, mask=None, training=None):\n",
        "      Performs a forward pass on the input and returns the output.\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, **kwargs):\n",
        "    \"\"\"\n",
        "    Initializes a new instance of the BaseAttention layer class.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    kwargs: dict\n",
        "        keyword arguments that will be passed to the MultiHeadAttention layer during initialization.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.ad_layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XqTJpaytFL9Y"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(BaseAttention):\n",
        "  \"\"\"\n",
        "  A class that implements cross-attention mechanism by inheriting from BaseAttention class.\n",
        "  Cross-attention is used to process two different sequences and attends to the context sequence while processing the query sequence.\n",
        "  Inherits:\n",
        "      BaseAttention: A base class that defines the MultiHeadAttention layer, LayerNormalization, and Add operation.\n",
        "  Args:\n",
        "      **kwargs: Arguments to pass to the MultiHeadAttention layer.\n",
        "  \"\"\"\n",
        "  def call(self, x, context,cnn):\n",
        "    \"\"\"\n",
        "    The call function that performs the cross-attention operation.\n",
        "\n",
        "    Args:\n",
        "        x: The query sequence tensor, shape=(batch_size, seq_len, embedding_dim)\n",
        "        context: The context sequence tensor, shape=(batch_size, seq_len, embedding_dim)\n",
        "\n",
        "    Returns:\n",
        "        The attended output tensor, shape=(batch_size, seq_len, embedding_dim)\n",
        "    \"\"\"\n",
        "    context=self.ad_layernorm(self.add([context, cnn]))\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        key=context,\n",
        "        value=context,\n",
        "        return_attention_scores=True)\n",
        "\n",
        "    # Cache the attention scores for plotting later.\n",
        "    self.last_attn_scores = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ShMPirQIFNIi"
      },
      "outputs": [],
      "source": [
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Apply the global self-attention mechanism to the input sequence.\n",
        "\n",
        "    Args:\n",
        "        x: A tensor of shape `(batch_size, seq_len, embedding_dim)`\n",
        "        representing the input sequence.\n",
        "\n",
        "    Returns:\n",
        "        A tensor of the same shape as the input, representing the sequence\n",
        "        after being transformed by the self-attention mechanism.\n",
        "    \"\"\"\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6jni0iZhFRue"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(BaseAttention):\n",
        "  \"\"\"\n",
        "  Call self attention on the input sequence, ensuring that each position in the\n",
        "  output depends only on previous positions (i.e. a causal model).\n",
        "\n",
        "  Args:\n",
        "      x: Input sequence tensor of shape `(batch_size, seq_len, embed_dim)`.\n",
        "\n",
        "  Returns:\n",
        "      Output sequence tensor of the same shape as the input, after self-attention\n",
        "      and residual connection with layer normalization applied.\n",
        "  \"\"\"\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2J3Xxm5lFVuh"
      },
      "outputs": [],
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  Implements the feedforward sublayer of the transformer block.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  d_model: int\n",
        "      The number of expected features in the input and output.\n",
        "  dff: int\n",
        "      The number of neurons in the first Dense layer.\n",
        "  dropout_rate: float, optional (default=0.1)\n",
        "      The dropout rate to use.\n",
        "\n",
        "  Attributes:\n",
        "  -----------\n",
        "  seq: tf.keras.Sequential\n",
        "      The sequential model that applies the two Dense layers and Dropout.\n",
        "  add: tf.keras.layers.Add\n",
        "      The addition layer that adds the residual connection.\n",
        "  layer_norm: tf.keras.layers.LayerNormalization\n",
        "      The normalization layer applied to the output.\n",
        "\n",
        "  Methods:\n",
        "  --------\n",
        "  call(x):\n",
        "      Computes the feedforward sublayer on the input tensor x and returns the output.\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Passes the input tensor `x` through a feedforward network consisting of two\n",
        "    dense layers with `dff` hidden units and a `relu` activation function.\n",
        "    A `dropout_rate` is applied after the first dense layer to prevent overfitting.\n",
        "    The output of the feedforward network is added to the original input `x` via the\n",
        "    `Add()` layer. Finally, the output is normalized using the `LayerNormalization()` layer.\n",
        "\n",
        "    Args:\n",
        "        x (tf.Tensor): Input tensor with shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: Output tensor with shape `(batch_size, seq_len, d_model)`.\n",
        "    \"\"\"\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zbA_gYjfFd4J"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  A single layer in the transformer encoder stack.\n",
        "\n",
        "  Args:\n",
        "    d_model (int): The dimensionality of the input and output sequences.\n",
        "    num_heads (int): The number of attention heads to be used in the self-attention sub-layer.\n",
        "    dff (int): The number of hidden units in the feedforward sub-layer.\n",
        "    dropout_rate (float): The dropout rate to be applied after the self-attention sub-layer.\n",
        "\n",
        "  Attributes:\n",
        "    self_attention (GlobalSelfAttention): A self-attention layer.\n",
        "    ffn (FeedForward): A feedforward neural network layer.\n",
        "  \"\"\"\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Applies the forward pass of the encoder layer.\n",
        "\n",
        "    Args:\n",
        "      x (tf.Tensor): The input sequence tensor.\n",
        "\n",
        "    Returns:\n",
        "      tf.Tensor: The output sequence tensor.\n",
        "    \"\"\"\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7Bl9f3LBFoPT"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  A custom Keras layer that implements the encoder of a transformer-based\n",
        "  neural network architecture for natural language processing tasks such\n",
        "  as language translation or text classification.\n",
        "\n",
        "  Args:\n",
        "    num_layers (int): The number of layers in the encoder.\n",
        "    d_model (int): The dimensionality of the output space.\n",
        "    num_heads (int): The number of attention heads in the multi-head\n",
        "      self-attention mechanism.\n",
        "    dff (int): The dimensionality of the fully connected feedforward\n",
        "      network.\n",
        "    vocab_size (int): The size of the vocabulary of the input language.\n",
        "    dropout_rate (float): The dropout rate to use for regularization.\n",
        "\n",
        "  Attributes:\n",
        "    d_model (int): The dimensionality of the output space.\n",
        "    num_layers (int): The number of layers in the encoder.\n",
        "    pos_embedding (PositionalEmbedding): The layer that learns the position\n",
        "      embeddings for each token in the input sequence.\n",
        "    enc_layers (list): A list of `EncoderLayer` instances, one for each\n",
        "      layer in the encoder architecture.\n",
        "    dropout (Dropout): The dropout layer for regularization.\n",
        "\n",
        "  Methods:\n",
        "    call(x): The forward pass of the encoder layer.\n",
        "\n",
        "  Returns:\n",
        "    The output tensor of the encoder layer, which has shape\n",
        "    `(batch_size, seq_len, d_model)`.\n",
        "  \"\"\"\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1,maxlen):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, d_model=d_model,maxlen=maxlen)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Perform forward pass of the `Encoder` layer.\n",
        "\n",
        "    Args:\n",
        "    x: tensor of shape (batch_size, sequence_length) representing the input token IDs sequence.\n",
        "\n",
        "    Returns:\n",
        "    A tensor of shape (batch_size, sequence_length, d_model) representing the output after applying\n",
        "    the self-attention and feed-forward layers to the input sequence.\n",
        "    \"\"\"\n",
        "    # `x` is token-IDs shape: (batch, seq_len)\n",
        "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "    # Add dropout.\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "\n",
        "    return x  # Shape `(batch_size, seq_len, d_model)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZZ_WAyQ3Fxuq"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  A single layer of the decoder in a transformer-based architecture.\n",
        "\n",
        "  Args:\n",
        "    d_model (int): The number of expected features in the input.\n",
        "    num_heads (int): The number of attention heads.\n",
        "    dff (int): The dimensionality of the feedforward network.\n",
        "    dropout_rate (float): The dropout rate to be applied.\n",
        "\n",
        "  Attributes:\n",
        "    causal_self_attention: An instance of the `CausalSelfAttention` layer.\n",
        "    cross_attention: An instance of the `CrossAttention` layer.\n",
        "    ffn: An instance of the `FeedForward` layer.\n",
        "    last_attn_scores: A tensor containing the last attention scores.\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               *,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x, context,cnn):\n",
        "    \"\"\"\n",
        "    Forward pass of the `DecoderLayer`.\n",
        "\n",
        "    Args:\n",
        "      x (tf.Tensor): The input tensor of shape\n",
        "      `(batch_size, target_seq_len, d_model)`.\n",
        "      context (tf.Tensor): The context tensor of shape\n",
        "      `(batch_size, input_seq_len, d_model)`.\n",
        "\n",
        "    Returns:\n",
        "      The output tensor of the `DecoderLayer` of shape\n",
        "      `(batch_size, target_seq_len, d_model)`.\n",
        "\n",
        "    \"\"\"\n",
        "    x = self.causal_self_attention(x=x)\n",
        "    x = self.cross_attention(x=x, context=context,cnn=cnn)\n",
        "\n",
        "    # Cache the last attention scores for plotting later\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VQ9okOHIF-yx"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  \"\"\"A decoder model for sequence to sequence learning.\n",
        "\n",
        "  This class implements a decoder layer for a transformer-based model used for sequence to sequence learning tasks. The decoder layer takes input embeddings, positional encodings, and attention masks as input, and returns the output of the decoder layer after applying a multi-head self-attention mechanism, followed by a cross-attention mechanism with the output from the encoder layers, and then applying a feed-forward neural network.\n",
        "\n",
        "  Attributes:\n",
        "    d_model (int): The number of output dimensions for each layer.\n",
        "    num_layers (int): The number of layers in the decoder.\n",
        "    pos_embedding (PositionalEmbedding): The positional embedding layer.\n",
        "    dropout (Dropout): A dropout layer.\n",
        "    dec_layers (list): A list of DecoderLayer objects.\n",
        "    last_attn_scores (ndarray): The attention scores from the last decoder layer.\n",
        "\n",
        "  Methods:\n",
        "    call(x, context): Implements the forward pass for the decoder layer.\n",
        "      Args:\n",
        "        x (ndarray): A tensor of shape (batch_size, target_seq_len), representing the input token IDs.\n",
        "        context (ndarray): A tensor of shape (batch_size, input_seq_len, d_model), representing the output from the encoder layers.\n",
        "      Returns:\n",
        "        ndarray: A tensor of shape (batch_size, target_seq_len, d_model), representing the output from the decoder layers.\n",
        "  \"\"\"\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1,maxlen):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
        "                                             d_model=d_model,\n",
        "                                             maxlen=maxlen)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
        "                     dff=dff, dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, context,cnn):\n",
        "    \"\"\"\n",
        "    Implements the forward pass for the decoder layer.\n",
        "\n",
        "    Args:\n",
        "      x (ndarray): A tensor of shape (batch_size, target_seq_len), representing the input token IDs.\n",
        "      context (ndarray): A tensor of shape (batch_size, input_seq_len, d_model), representing the output from the encoder layers.\n",
        "\n",
        "    Returns:\n",
        "      ndarray: A tensor of shape (batch_size, target_seq_len, d_model), representing the output from the decoder layers.\n",
        "    \"\"\"\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, context,cnn)\n",
        "\n",
        "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "\n",
        "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1671gweFrFMJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GDzPlPUWt8aQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ksfYBfmyGLdo",
        "outputId": "a94a90ff-58b2-475d-c3eb-44461779c40f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "\n",
        "## Encoder\n",
        "encoder_inputs_img = keras.Input(shape=(128,128,3), name=\"CNN_inputs\")\n",
        "encoder_inputs_txt = keras.Input(shape=(SEQUENCE_LENGTH), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "\n",
        "\n",
        "encoder_outputs = Encoder(d_model=EMBEDDING_SIZE,\n",
        "                          dff=FF_EXPANSION,\n",
        "                          num_heads=HEADS,\n",
        "                          vocab_size=VOCAB_SIZE,\n",
        "                          num_layers=1,\n",
        "                          maxlen=SEQUENCE_LENGTH)(encoder_inputs_txt)\n",
        "\n",
        "\n",
        "cnn=tf.keras.applications.InceptionV3(input_shape=(128,128,3),\n",
        "\n",
        "                                                         include_top=False,\n",
        "                                                        weights='imagenet',\n",
        "                                                        input_tensor=encoder_inputs_img)(encoder_inputs_img)\n",
        "cnn=layers.Flatten()(cnn)\n",
        "cnn=layers.Dense(FF_EXPANSION,activation='relu')(cnn)\n",
        "cnn=layers.RepeatVector(SEQUENCE_LENGTH)(cnn)\n",
        "cnn=layers.Dense(EMBEDDING_SIZE,activation='relu')(cnn)\n",
        "cnn=layers.BatchNormalization()(cnn)\n",
        "\n",
        "\n",
        "\n",
        "## Decoder\n",
        "decoder_inputs = keras.Input(shape=(SEQUENCE_LENGTH), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "\n",
        "\n",
        "\n",
        "#x=layers.add(cnn,encoder_outputs)\n",
        "x = Decoder(d_model=EMBEDDING_SIZE,\n",
        "           dff=FF_EXPANSION,\n",
        "           num_heads=HEADS,\n",
        "           vocab_size=VOCAB_SIZE,\n",
        "           num_layers=1,\n",
        "           maxlen=SEQUENCE_LENGTH)(decoder_inputs,encoder_outputs,cnn)\n",
        "\n",
        "decoder_outputs = layers.Dense(VOCAB_SIZE,activation='softmax')(x) # 15000 token\n",
        "#decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "## Transformer Model\n",
        "#decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs_img,encoder_inputs_txt,decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xRPZZ5MuwoXN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "durRNywLt8Y3",
        "outputId": "eb7e8cb1-07a7-4568-f3c9-973ae384f204"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7a3bb153b670>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoint_filepath='/content/drive/MyDrive/VQA_model_weights_v2/tmp/checkpoint'\n",
        "transformer.load_weights(checkpoint_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0jHbVdT_t8WB"
      },
      "outputs": [],
      "source": [
        "def get_ans(img,question,encoder,model,max_length):\n",
        "    trans_input=tf.keras.utils.pad_sequences(np.array(encoder.encode(question)).reshape(1,-1),\n",
        "                              padding='post',maxlen=max_length)\n",
        "    start_end = encoder.encode(['[sos] [eos]'])\n",
        "    start = start_end[0][tf.newaxis]\n",
        "    end = start_end[1][tf.newaxis]\n",
        "\n",
        "    # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
        "    # dynamic-loop can be traced by `tf.function`.\n",
        "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "    output_array = output_array.write(0, start)\n",
        "\n",
        "    for i in tf.range(max_length):\n",
        "      output = tf.transpose(output_array.stack())\n",
        "      predictions = transformer.predict((img,trans_input, output), training=False)\n",
        "\n",
        "      # Select the last token from the `seq_len` dimension.\n",
        "      #predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
        "      print(predictions.shape)\n",
        "      print(predictions)\n",
        "      predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "      # Concatenate the `predicted_id` to the output which is given to the\n",
        "      # decoder as its input.\n",
        "      output_array = output_array.write(i+1, predicted_id[0])\n",
        "\n",
        "\n",
        "\n",
        "    output = tf.transpose(output_array.stack())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3Sz5OI4D3kgB"
      },
      "outputs": [],
      "source": [
        "def get_ans2(img,question,encoder,model,max_length):\n",
        "    trans_input=tf.keras.utils.pad_sequences(np.array(encoder.encode(question)).reshape(1,-1),\n",
        "                              padding='post',maxlen=max_length)\n",
        "    start_end = encoder.encode('[sos] [eos]')\n",
        "    start = 0\n",
        "    end = start_end[1]\n",
        "\n",
        "    # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
        "    # dynamic-loop can be traced by `tf.function`.\n",
        "    output_array = np.zeros_like(trans_input)\n",
        "    output_array[:,0]=start\n",
        "   # output_array = output_array.write(0, start)\n",
        "    #img=  cv.resize(cv.imread(img), (128,128)).astype('float32') / 255.\n",
        "    img=np.array(img).reshape((1,*img.shape))\n",
        "   # trans_input=trans_input.reshape((1,*trans_input.shape))\n",
        "   # output_array=output_array.reshape((1,*output_array.shape))\n",
        "    for i in range(max_length-1):\n",
        "      output = output_array\n",
        "      predictions = model.predict((img,trans_input, output))\n",
        "\n",
        "      # Select the last token from the `seq_len` dimension.\n",
        "\n",
        "\n",
        "      predicted_id = tf.argmax(predictions, axis=-1)\n",
        "      #print(predicted_id)\n",
        "\n",
        "      # Concatenate the `predicted_id` to the output which is given to the\n",
        "      # decoder as its input.\n",
        "      print(encoder.decode(predicted_id[0]))\n",
        "\n",
        "      output_array[:,i+1]=predicted_id[0][i]\n",
        "\n",
        "\n",
        "\n",
        "    #print(output_array[0])\n",
        "    return encoder.decode(output_array[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oi6YSA18dKYY",
        "outputId": "680e3326-8876-4e90-d7cd-94dc057e969e"
      },
      "outputs": [
        {
          "ename": "error",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-53b9d32a4460>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m answer = get_ans2(img=cv.resize(cv.imread('/content/Jude-Bellingham-Real-Madrid-1.jpg'), (128,128)).astype('float32') / 255.,\n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Do you know this ?'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mencoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     max_length=SEQUENCE_LENGTH)\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) /io/opencv/modules/imgproc/src/resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
          ]
        }
      ],
      "source": [
        "  answer = get_ans2(img=cv.resize(cv.imread('/content/Jude-Bellingham-Real-Madrid-1.jpg'), (128,128)).astype('float32') / 255.,\n",
        "                      question='Do you know this ?',\n",
        "                      encoder=encoder,\n",
        "                      model=transformer,\n",
        "                      max_length=SEQUENCE_LENGTH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4haU16QFeQ76"
      },
      "outputs": [],
      "source": [
        "answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6TC5X2iuW_7t",
        "outputId": "47b37a35-9111-4b1a-acbf-727d02f0a60b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://ba3a4bfbcb751c182e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://ba3a4bfbcb751c182e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 4s 4s/step\n",
            "shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley\n",
            "1/1 [==============================] - 0s 131ms/step\n",
            "shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley shipley\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import tensorflow as tf\n",
        "#0from transformers import AutoTokenizer, TFLxmertModel\n",
        "\n",
        "# Load the LXMERT model and tokenizer\n",
        "#model_name = \"unc-nlp/lxmert-base-uncased\"  # Example LXMERT model\n",
        "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#model = TFLxmertModel.from_pretrained(model_name)\n",
        "\n",
        "def preprocess_image(image):\n",
        "    # Resize and preprocess the image as needed\n",
        "    # Replace this with your image preprocessing logic\n",
        "    resized_image = tf.image.resize(image, (128, 128))\n",
        "    preprocessed_image = resized_image / 255.0  # Normalize to [0, 1]\n",
        "    return preprocessed_image\n",
        "\n",
        "\n",
        "\n",
        "def answer_question(image, question):\n",
        "    # Preprocess the image and tokenize the question\n",
        "    preprocessed_image = preprocess_image(image)\n",
        "\n",
        "    answer = get_ans2(img=preprocessed_image,\n",
        "                      question=question,\n",
        "                      encoder=encoder,\n",
        "                      model=transformer,\n",
        "                      max_length=SEQUENCE_LENGTH)\n",
        "\n",
        "    # Extract the answer from the model's output\n",
        "    return answer\n",
        "\n",
        "# Create the Gradio interface\n",
        "iface = gr.Interface(fn=answer_question, inputs=[\"image\", \"text\"], outputs=\"text\")\n",
        "iface.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}